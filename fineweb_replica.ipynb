{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset # pip install datasets\n",
    "from tqdm import tqdm # pip install tqdm\n",
    "from utils import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load dataset from huggingface\n",
    "\n",
    "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80cf82131324ca88a26704b26c51f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1cded4ebe4448090a0d3e08fb64f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object type <class 'datasets.arrow_dataset.Dataset'>\n",
      "# of data points 9672101\n",
      "one data point\n",
      "{'text': 'The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independence of thought and the freedom to choose.\\nElizabeth’s refusal of Mr. Collins offer of marriage showed an independence seldom seen in heroines of the day. Her refusal of Mr. Darcy while triggered by anger showed a level of independence that left him shocked and stunned.\\nThe freedom she exhibited in finally accepting him in direct defiance of Lady Catherine and knowing her father would disapprove was unusual even for Austen. In her last book Anne Elliot is persuaded to refuse Captain Wentworth at Lady Russel’s insistence.\\nAlthough Jane played by the rules of the day, all of her writing is infused with how she wanted life to be. She ‘screams’ her outrage at the limitations for women in Emma.\\nWhen accosted by Mrs. Elton, Jane Fairfax says,\\n“Excuse me, ma’am, but this is by no means my intention; I make no inquiry myself, and should be sorry to have any made by my friends. When I am quite determined as to the time, I am not at all afraid of being long unemployed. There are places in town, offices, where inquiry would soon produce something — offices for the sale, not quite of human flesh, but of human intellect.”\\n“Oh! my dear, human flesh! You quite shock me; if you mean a fling at the slave-trade, I assure you Mr. Suckling was always rather a friend to the abolition.”\\n“I did not mean, I was not thinking of the slave-trade,” replied Jane; “governess-trade, I assure you, was all that I had in view; widely different certainly, as to the guilt of those who carry it on; but as to the greater misery of the victims, I do not know where it lies.”\\nThat same sentiment is emphasized in Emma’s shock when Mrs. Weston tells her of Frank Churchill’s secret engagement to Jane.\\n“Good God!” cried Emma, “Jane actually on the point of going as governess! What could he mean by such horrible indelicacy? To suffer her to engage herself — to suffer her even to think of such a measure!”\\nI find it interesting that at the moment of Austen’s birth or there about, John Adams left his farm in Massachusetts for the Continental Congress in Philadelphia. Doesn’t sound particularly interesting, I know but consider this.\\nJohn Adams left his home in mid-December 1775 to attend an unprecedented meeting of colonial representatives to consider severing ties with their mother country and her monarch; a decision that culminated in a document unlike any ever written. In the mother country, one day in that same cold December a baby girl was born at Steventon Rectory. Her cry was heard by only the people in the house but the years to come would see her pen create works unlike any the world had ever seen.\\nComparing Austen’s words with Thomas Jefferson’s may seem a trivialization but I believe that Austen’s impact on the world is no less important than Jefferson’s. The effect of Jane’s writing maybe more subtle than that of the Virginian but it is no less influential.\\nJefferson’s words instigated and promoted a revolution, a war of independence. Jane’s words had no such excessive consequence. Still in her own quiet, genteel yet powerful way she declared and promoted the same principles of freedom and self-regulated independence as our American forefathers. In all her novels Jane advocates independence of person and thought, the rights of all and acceptance of responsibility for those rights.\\nJane may not have incited military action as Jefferson did but even as an avowed royalist, I doubt not that Jane Austen firmly believed in his declaration of the right to life, liberty and the pursuit of happiness.', 'id': '<urn:uuid:0d8a309d-25c5-405d-a08a-c11239f0d717>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://austenauthors.net/the-independent-jane', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9743200540542603, 'token_count': 845, 'score': 2.75, 'int_score': 3}\n"
     ]
    }
   ],
   "source": [
    "local_dir = \"edu_fineweb10B\"\n",
    "shard_size = int(1e7)\n",
    "\n",
    "# create the repo for data\n",
    "DATA_CACHE_DIR = local_dir\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# download fw data\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\")\n",
    "\n",
    "print(\"object type\", type(fw))\n",
    "print(\"# of data points\", len(fw))\n",
    "print(\"one data point\\n\", fw[0], sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 300,000 data points, about 3 shards\n",
    "n_examples = 100000\n",
    "\n",
    "fw_sample = []\n",
    "for i in range(n_examples):\n",
    "    fw_sample.append(fw[i])\n",
    "\n",
    "len(fw_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of text token is 50256\n",
      "encoding of hello world is [31373, 995, 0]\n",
      "max token value is 50256\n"
     ]
    }
   ],
   "source": [
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "\n",
    "# learn about tokenizer\n",
    "print('end of text token is', eot)\n",
    "print('encoding of hello world is', enc.encode_ordinary('hello world!')) # note: that encode_ordinary's input is text string\n",
    "\n",
    "max_token_value = enc.max_token_value\n",
    "print('max token value is', max_token_value ) \n",
    "\n",
    "assert  max_token_value  < 2**16, \"token dictionary too large for uint16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move tokenize to utils.py to ensure it is in __main__\n",
    "# def tokenize(doc):\n",
    "#     \"\"\"\n",
    "#     doc: string of a single document\n",
    "#     returns a numpy array of unit 16 tokens\n",
    "#     \"\"\"\n",
    " \n",
    "#     tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
    "#     tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "#     tokens_np = np.array(tokens)\n",
    "\n",
    "#     ## note: remove below token check, checking once is enough \n",
    "#     ## assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "   \n",
    "#     tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "#     return tokens_np_uint16\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in each data point 869\n",
      "# of data points in each shard 11507\n"
     ]
    }
   ],
   "source": [
    "# each example has about 800-1000 tokens\n",
    "n_tokens = len(enc.encode_ordinary(fw_sample[5]['text']))\n",
    "print(\"# of tokens in each data point\", n_tokens )\n",
    "print(\"# of data points in each shard\", shard_size//n_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. write 1d array to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of cores for compute 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|██████████| 10000000/10000000 [00:00<00:00, 21163622.20tokens/s]\n",
      "Shard 1: 100%|█████████▉| 9999759/10000000 [00:00<00:00, 21666308.08tokens/s]\n",
      "Shard 2: 100%|█████████▉| 9999953/10000000 [00:00<00:00, 21849361.56tokens/s]\n",
      "Shard 3: 100%|█████████▉| 9999193/10000000 [00:00<00:00, 21676929.02tokens/s]\n",
      "Shard 4: 100%|█████████▉| 9999286/10000000 [00:00<00:00, 21540302.13tokens/s]\n",
      "Shard 5: 100%|█████████▉| 9997276/10000000 [00:00<00:00, 21852983.87tokens/s]\n",
      "Shard 6: 100%|█████████▉| 9999095/10000000 [00:00<00:00, 21658100.61tokens/s]\n",
      "Shard 7: 100%|█████████▉| 9999599/10000000 [00:00<00:00, 21713272.98tokens/s]\n",
      "Shard 8: 100%|█████████▉| 9999578/10000000 [00:00<00:00, 21638808.96tokens/s]\n",
      "Shard 9: 100%|█████████▉| 9999914/10000000 [00:00<00:00, 21292991.58tokens/s]\n",
      "Shard 10:  44%|████▍     | 4435500/10000000 [00:00<00:00, 21866065.89tokens/s]"
     ]
    }
   ],
   "source": [
    "nprocs = max(1, os.cpu_count()//2)\n",
    "print(\"# of cores for compute\", nprocs)\n",
    "\n",
    "# write tokens to shards using cpu parallel computation\n",
    "with mp.Pool(nprocs) as pool:\n",
    "\n",
    "    shard_index = 0\n",
    "\n",
    "    token_count = 0 # already tokenized - yet to writte count\n",
    "    all_tokens_np = np.empty((shard_size, ), dtype = np.uint16)\n",
    "\n",
    "    progress_bar = None\n",
    "\n",
    "    for tokens in pool.imap(tokenize, fw_sample, chunksize=8):\n",
    "        \n",
    "        # not need to create a new shard\n",
    "        if token_count + len(tokens) < shard_size: \n",
    "            \n",
    "            all_tokens_np[token_count: token_count + len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total = shard_size, unit = 'tokens', desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        \n",
    "        # need to create a new shard\n",
    "        else:\n",
    "            \n",
    "            split = 'val' if shard_index == 0 else 'train'\n",
    "            filename = os.path.join(DATA_CACHE_DIR, f'edufineweb_{split}_{shard_index:06d}')\n",
    "\n",
    "            remainder = shard_size - token_count \n",
    "            all_tokens_np[token_count:] = tokens[:remainder]\n",
    "            \n",
    "            write_datafile(filename, all_tokens_np)\n",
    "            progress_bar.update(remainder)\n",
    "            \n",
    "            # initiate for the next shard\n",
    "            all_tokens_np = np.empty((shard_size, ), dtype = np.uint16)\n",
    "            shard_index += 1\n",
    "\n",
    "            token_count = len(tokens) - remainder\n",
    "            all_tokens_np[:token_count] = tokens[remainder: ] \n",
    "            \n",
    "            progress_bar = None \n",
    "    \n",
    "\n",
    "    if token_count > 0:\n",
    "        split = 'val' if shard_index == 0 else 'train'\n",
    "        filename = os.path.join(DATA_CACHE_DIR, f'edufineweb_{split}_{shard_index:06d}')\n",
    "        write_datafile(filename, all_tokens_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 10:  45%|████▌     | 4500787/10000000 [00:15<00:00, 21866065.89tokens/s]"
     ]
    }
   ],
   "source": [
    "# nprocs = max(1, os.cpu_count()//2)\n",
    "# print(\"# of cores for compute\", nprocs)\n",
    "\n",
    "# # write tokens to shards using cpu parallel computation\n",
    "# progress_bar = None\n",
    "# shard_index = 0\n",
    "\n",
    "# token_count = 0 # already tokenized - yet to writte count\n",
    "# all_tokens_np = np.empty((shard_size, ), dtype = np.uint16)\n",
    "\n",
    "# for tokens in map(tokenize, fw_sample):\n",
    "        \n",
    "#     # not need to create a new shard\n",
    "#     if token_count + len(tokens) < shard_size: \n",
    "        \n",
    "#         all_tokens_np[token_count: token_count + len(tokens)] = tokens\n",
    "#         token_count += len(tokens)\n",
    "#         if progress_bar is None:\n",
    "#             progress_bar = tqdm(total = shard_size, unit = 'tokens', desc=f\"Shard {shard_index}\")\n",
    "#         progress_bar.update(len(tokens))\n",
    "    \n",
    "#     # need to create a new shard\n",
    "#     else:\n",
    "        \n",
    "#         split = 'val' if shard_index == 0 else 'train'\n",
    "#         filename = os.path.join(DATA_CACHE_DIR, f'edufineweb_{split}_{shard_index:06d}')\n",
    "\n",
    "#         remainder = shard_size - token_count \n",
    "#         all_tokens_np[token_count:] = tokens[:remainder]\n",
    "        \n",
    "#         print(token_count, len(tokens), progress_bar)\n",
    "#         write_datafile(filename, all_tokens_np)\n",
    "#         progress_bar.update(remainder)\n",
    "        \n",
    "#         # initiate for the next shard\n",
    "#         shard_index += 1\n",
    "#         all_tokens_np = np.empty((shard_size, ), dtype = np.uint16)\n",
    "\n",
    "#         token_count = len(tokens) - remainder\n",
    "#         all_tokens_np[:token_count] = tokens[remainder: ] \n",
    "#         progress_bar = None \n",
    "    \n",
    "\n",
    "#     if token_count > 0:\n",
    "#         split = 'val' if shard_index == 0 else 'train'\n",
    "#         filename = os.path.join(DATA_CACHE_DIR, f'edufineweb_{split}_{shard_index:06d}')\n",
    "#         write_datafile(filename, all_tokens_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
