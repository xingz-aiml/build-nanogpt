{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce data load in train_gpt2.py\n",
    "\n",
    "**learnings**\n",
    "\n",
    "1: a way to add additional attribute to module: self.layer_name.attribute_name = value\n",
    "2: the last layer bias == False to be the same as pretrained models from HF\n",
    "3: use classmethod add from_pretrained method\n",
    "4: a way to inspect input arguments: inspect.signature(torch.optim.AdamW).parameters\n",
    "\n",
    "**questions**\n",
    "\n",
    "1: no dropout in his defined gpt\n",
    "- none in nn.scaled_dot_product_attention which is controlled by dropout_p param and defaults to 0\n",
    "- none else where still about dropout\n",
    "\n",
    "2: do we need to use _copy to copy weights?\n",
    "\n",
    "3: why only 2d weights uses weight-decay (L2), bias and layer norm do not\n",
    "\n",
    "4: grad_accum_steps is for n_iter for each mini_batch, how to determine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edufineweb_train_000008.npy',\n",
       " 'edufineweb_val_000000.npy',\n",
       " 'edufineweb_train_000009.npy',\n",
       " 'edufineweb_train_000001.npy',\n",
       " 'edufineweb_train_000002.npy',\n",
       " 'edufineweb_train_000003.npy',\n",
       " 'edufineweb_train_000007.npy',\n",
       " 'edufineweb_train_000006.npy',\n",
       " 'edufineweb_train_000004.npy',\n",
       " 'edufineweb_train_000010.npy',\n",
       " 'edufineweb_train_000005.npy']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "import math \n",
    "import pandas as pd\n",
    "import inspect\n",
    "\n",
    "data_dir = 'edu_fineweb10B'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'params': <Parameter \"params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]]\">, 'lr': <Parameter \"lr: Union[float, torch.Tensor] = 0.001\">, 'betas': <Parameter \"betas: Tuple[float, float] = (0.9, 0.999)\">, 'eps': <Parameter \"eps: float = 1e-08\">, 'weight_decay': <Parameter \"weight_decay: float = 0.01\">, 'amsgrad': <Parameter \"amsgrad: bool = False\">, 'maximize': <Parameter \"maximize: bool = False\">, 'foreach': <Parameter \"foreach: Optional[bool] = None\">, 'capturable': <Parameter \"capturable: bool = False\">, 'differentiable': <Parameter \"differentiable: bool = False\">, 'fused': <Parameter \"fused: Optional[bool] = None\">})\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(inspect.signature(torch.optim.AdamW).parameters) # learning 4: a way to inspect input arguments\n",
    "\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "print(fused_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available: False\n",
      "is mps available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"is cuda available:\", torch.cuda.is_available())\n",
    "print(\"is mps available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())\n",
    "\n",
    "device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "\n",
    "# print(\"device:\", device)\n",
    "# device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\" # learning 2: set device_type as well just in case\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.config = config \n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1  ## learning 1: a way to add additional attribute to module\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.config.n_embd, dim = 2)\n",
    "\n",
    "        q = q.contiguous().view(B,T,self.config.n_head, -1).transpose(1,2)\n",
    "        k = k.contiguous().view(B,T,self.config.n_head, -1).transpose(1,2)\n",
    "        v = v.contiguous().view(B,T,self.config.n_head, -1).transpose(1,2)\n",
    "\n",
    "        y =  F.scaled_dot_product_attention(q, k, v, is_causal = True) ## question 1: no dropout, which is controlled by dropout_p param and defaults to 0\n",
    "        y = y.transpose(1,2).contiguous().view(B, T, -1)\n",
    "        y = self.c_proj(y)                \n",
    "\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(config.n_embd*4 , config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x) ## question 1: no dropout after gelu\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x+self.attn(self.ln_1(x))\n",
    "        x = x+self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    n_embd: int = 768\n",
    "    n_layer: int = 2\n",
    "    n_head: int = 12\n",
    "    vocab_size:int = 50257 # https://chatgpt.com/share/67a85058-8878-8004-9bc7-374a9325eeea\n",
    "    block_size:int = 1024 # max sequence length\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        )\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # learning 2: the last layer bias == False to be the same as pretrained models from HF\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights) ## this do not understand\n",
    "\n",
    "    def _init_weights(self, module):  ## check init weight, not used in the script\n",
    "\n",
    "        std = 0.02\n",
    "\n",
    "        if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "            std *= (2*self.config.n_layer)**-0.5 # note when n_layer = 1, scale = 1/sqrt(2)\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "\n",
    "        B, T = x.shape\n",
    "\n",
    "        assert T < self.config.block_size, f\"can't forward sequence length of {T}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        x_pos = torch.arange(T, dtype = torch.long, device = x.device).expand_as(x)\n",
    "        x_pos = self.transformer.wpe(x_pos).to(device)\n",
    "        x_token = self.transformer.wte(x)\n",
    "        x = x_pos+x_token\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    " \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return x, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type): # learning 3: use classmethod add from_pretrained method\n",
    "        \"\"\"creates a model and load the pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "\n",
    "        \n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        \n",
    "        ## 1. create a model from-scratch\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        model_configs = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }\n",
    "\n",
    "        config_args = model_configs[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "\n",
    "        \n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "        \n",
    "        ## 2. load hf weights\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())  ## question 2: do we need to _copy \n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        \n",
    "        param_dict = {pn: p for pn, p in model_from_cratch.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2] ## question 3: why only 2d weights uses weight-decay (L2), bias and layer norm do not\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3283, -0.4139,  2.1732,  ..., -1.3381, -1.0170,  0.2321],\n",
       "          [-1.3573, -1.2284,  1.8616,  ..., -0.6131, -0.8520,  0.2109],\n",
       "          [-0.2146, -0.7371,  1.4238,  ..., -0.7926, -0.9502,  0.9302],\n",
       "          ...,\n",
       "          [-0.9639, -1.2737,  0.6931,  ..., -1.4538, -0.1192, -0.7945],\n",
       "          [-0.6201, -0.0045,  0.4539,  ..., -1.4145, -0.8857,  0.0270],\n",
       "          [-0.1606, -0.3282,  0.1709,  ..., -1.7915, -0.9196,  0.7598]],\n",
       " \n",
       "         [[-1.3283, -0.4139,  2.1732,  ..., -1.3381, -1.0170,  0.2321],\n",
       "          [-1.3573, -1.2284,  1.8616,  ..., -0.6131, -0.8520,  0.2109],\n",
       "          [-0.2146, -0.7371,  1.4238,  ..., -0.7926, -0.9502,  0.9302],\n",
       "          ...,\n",
       "          [-0.9639, -1.2737,  0.6931,  ..., -1.4538, -0.1192, -0.7945],\n",
       "          [-0.6201, -0.0045,  0.4539,  ..., -1.4145, -0.8857,  0.0270],\n",
       "          [-0.1606, -0.3282,  0.1709,  ..., -1.7915, -0.9196,  0.7598]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTConfig.n_layer = 12\n",
    "GPTConfig.n_head = 12\n",
    "GPTConfig.n_embd=768\n",
    "\n",
    "x = torch.arange(10).expand(2,10).to(device)\n",
    "print(x)\n",
    "model_from_cratch = GPT(GPTConfig)\n",
    "model_from_cratch(x, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf weight shape: torch.Size([768, 2304])\n",
      "our weight shape: torch.Size([2304, 768])\n",
      "model pretrained\n",
      " GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "model from scratch\n",
      " GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='tanh')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# use one example to see the difference between weights from huggingface and here\n",
    "model_type = 'gpt2'\n",
    "from transformers import GPT2LMHeadModel\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "sd_hf = model_hf.state_dict()\n",
    "# transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "# keys = [k for k in sd_hf if any(k.endswith(t) for t in transposed)]\n",
    "\n",
    "print(\"hf weight shape:\", sd_hf['transformer.h.0.attn.c_attn.weight'].shape)\n",
    "sd = model_from_cratch.state_dict()\n",
    "\n",
    "print(\"our weight shape:\", sd['transformer.h.0.attn.c_attn.weight'].shape)\n",
    "\n",
    "print(\"model pretrained\\n\", model_hf)\n",
    "print(\"model from scratch\\n\", model_from_cratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader\n",
    "use the one in train_gpt2_1_dataloader because of no ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "\n",
    "    data = np.load(filename)\n",
    "    data = data.astype(np.int32)\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    return data_tensor\n",
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, split, data_dir):\n",
    "\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.split = split \n",
    "        assert split in {'train','val'}\n",
    "\n",
    "        self.shards = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if split in filename]\n",
    "        assert len(self.shards) > 0, \"no shards\"\n",
    "\n",
    "        print(f\"{len(self.shards)} shards for {split}\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T \n",
    "        n_tokens_in_batch = B*T\n",
    "        buf = self.tokens[self.current_position: self.current_position+n_tokens_in_batch+1]\n",
    "        x = buf[:-1].view(B,T)\n",
    "        y = buf[1:].view(B,T)\n",
    "\n",
    "        self.current_position += n_tokens_in_batch #??\n",
    "        \n",
    "        # if the remaining is not enough to be a batch, skip the remaining and advance to the next shard\n",
    "        if self.current_position + n_tokens_in_batch + 1 > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.current_position = 0\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_accum_steps is 15\n",
      "10 shards for train\n",
      "1 shards for val\n"
     ]
    }
   ],
   "source": [
    "# load encoder \n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "total_batch_size = 1000*1000 # 524288 == 2**19, ~0.5M, in number of tokens\n",
    "                         # n_texts * n_token_per_text \n",
    "B = 64 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "\n",
    "grad_accum_steps = total_batch_size // (B * T) # question 4: grad_accum_steps is for n_iter for each mini_batch, how to determine it\n",
    "print(\"grad_accum_steps is\", grad_accum_steps)\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, split=\"train\", data_dir=data_dir)\n",
    "val_loader = DataLoaderLite(B=B, T=T, split=\"val\",data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='x'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# did not use ddp because cuda is unavailable\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "\n",
    "max_steps = 10000*1000/(B*T)\n",
    "\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr*(it+1)/warmup_steps\n",
    "    elif it >max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "# the decay is non-linear\n",
    "x = np.linspace(0, math.pi, 1000)\n",
    "y = [math.cos(v) for v in x]\n",
    "df = pd.DataFrame({'x':x , 'y':y})\n",
    "df.plot(x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
