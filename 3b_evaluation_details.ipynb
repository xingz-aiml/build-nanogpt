{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- hella swag\n",
    "- sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import os\n",
    "import requests \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "encoder = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. sentence generation\n",
    "- learning 1: apply temperature to the logits to change distribution (not included in the original code)\n",
    "- learning 2: \n",
    "\n",
    "    - model.eval(): no dropout - important for good perf; batch norm mean is using stored values - does not apply here\n",
    "    - torch.no_grad(): no wasted compute and memory on gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What makes a person resilient is that they can be resilient. In order to be resilient, you have to be able to adapt to change.\\n\\nIf you're not strong\",\n",
       " \"What makes a person resilient?\\n\\nIf you're a person who's never been able to cope with a lot of stress, you can't always take the time to figure\",\n",
       " 'What makes a person resilient is how they can be resilient. If they are going to get through adversity, they have to be resilient. If they are going to get through adversity',\n",
       " \"What makes a person resilient?\\n\\nIn the first place, being resilient has more to do with the person's capacity for survival. If you're not going to die,\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # learning 2\n",
    "def complete_sentence(model, \n",
    "                      encoder, \n",
    "                      text: str, \n",
    "                      n_examples: int = 4, \n",
    "                      max_n_generated_tokens: int = 30, \n",
    "                      top_k_to_include_in_random_draw: int = 20, \n",
    "                      temperature: float = 0.6):\n",
    "    \n",
    "    was_training = model.training\n",
    "\n",
    "    model.eval() # learning 2\n",
    "\n",
    "    #with torch.no_grad():\n",
    "        \n",
    "        # text to tensor\n",
    "    tokens = encoder.encode_ordinary(text)\n",
    "    tensor = torch.tensor(tokens).unsqueeze(0).repeat(n_examples, 1).to(model.device) # B * T \n",
    "\n",
    "    for _ in range(max_n_generated_tokens):\n",
    "\n",
    "        # get probability for the next token\n",
    "        logits = model(tensor).logits[:, -1, :] \n",
    "        probs = F.softmax(logits / temperature, dim = -1) # learning 1\n",
    "        \n",
    "        # top k samples\n",
    "        top_probs, top_idx = torch.topk(probs, k = top_k_to_include_in_random_draw, dim=-1)   # B * k\n",
    "        selected_idx_on_top_probs = torch.multinomial(top_probs, 1) # B * 1\n",
    "        \n",
    "        next_tokens = torch.gather(top_idx, -1, selected_idx_on_top_probs) # B * 1\n",
    "        \n",
    "        # concat the new token with existing\n",
    "        tensor = torch.cat([tensor, next_tokens], dim = -1)\n",
    "\n",
    "    # decode\n",
    "    decoded = []\n",
    "    for i in range(n_examples):\n",
    "\n",
    "        tokens = tensor[i, :].tolist()\n",
    "        if encoder.eot_token in tokens:\n",
    "            tokens = tokens[: tokens.index(encoder.eot_token)]\n",
    "        \n",
    "        decoded.append(encoder.decode(tokens))\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return decoded\n",
    "\n",
    "complete_sentence(model, encoder, \"What makes a person resilient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. hellaswag\n",
    "learning 3: how to send requests to get data and save by chunk\n",
    "\n",
    "learning 4: use contiguous after slicing, which makes subsequent view and operations smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils to download hellaswag and write to file\n",
    "def _download_file(url: str, file_loc: str, chunk_size: int = 1024): # learning 3\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(file_loc, \"wb\") as file, tqdm(desc = file_loc, \n",
    "                                        total =  total, \n",
    "                                        unit = 'iB',\n",
    "                                        unit_scale = True,\n",
    "                                        unit_divisor = 1024) as bar:\n",
    "        \n",
    "        for data in resp.iter_content(chunk_size = chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "\n",
    "def download_hellaswag(split: str = 'train') -> None:\n",
    "\n",
    "    hellaswags = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",}\n",
    "\n",
    "\n",
    "    local_dir, filename = \"hellaswag\", f\"hellaswag_{split}.jsonl\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    file_loc = os.path.join(local_dir, filename)\n",
    "    url = hellaswags[split]\n",
    "\n",
    "    if not os.path.exists(file_loc):\n",
    "        print(f\"download hellaswag {url} to {file_loc}\")\n",
    "        _download_file(url, file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_hellaswag(split: str = 'val'):\n",
    "    download_hellaswag(split)\n",
    "    with open(f\"hellaswag/hellaswag_{split}.jsonl\", \"r\") as f:\n",
    "        n = 0\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            yield example\n",
    "            n += 1\n",
    "            if n >= 50:\n",
    "                break\n",
    "\n",
    "def render_example(example, encoder):\n",
    "\n",
    "    context  = example['ctx']\n",
    "    label = int(example['label'])\n",
    "    endings = example['endings']\n",
    "\n",
    "    # create tokens\n",
    "    context_tokens = encoder.encode_ordinary(context)\n",
    "    context_len = len(context_tokens)\n",
    "\n",
    "    masks = []\n",
    "    tokens = []\n",
    "    max_len = 0\n",
    "    for ending in endings:\n",
    "        ending_tokens = encoder.encode_ordinary(' ' + ending)\n",
    "        ending_len = len(ending_tokens)\n",
    "        max_len = max(max_len, context_len + ending_len)\n",
    "\n",
    "        masks.append([0]* context_len + [1]*ending_len)\n",
    "        tokens.append(context_tokens + ending_tokens)\n",
    "     \n",
    "    # convert to padded tensors\n",
    "    padded_masks = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    padded_tokens = torch.zeros((4, max_len), dtype =torch.long)\n",
    "\n",
    "    for i in range(4):\n",
    "        curr_len = len(tokens[i])\n",
    "        padded_masks[i, :curr_len] = torch.tensor(masks[i])\n",
    "        padded_tokens[i, :curr_len] = torch.tensor(tokens[i])\n",
    "\n",
    "    return padded_tokens, padded_masks, label\n",
    "\n",
    "# # test if usable\n",
    "# iterator = iter_hellaswag('val')\n",
    "# for example in iterator:\n",
    "#     tokens, masks, label = render_example(example, encoder)\n",
    "#     break \n",
    "\n",
    "# print(tokens, masks, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval_hellaswag(iterator, model, encoder):\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for example in iterator:\n",
    "        tokens, masks, label = render_example(example, encoder)\n",
    "\n",
    "        tokens = tokens.to(model.device)\n",
    "        x = tokens[:, :-1].contiguous()\n",
    "        y = tokens[:, 1:].contiguous()         \n",
    "        masks = masks.to(model.device)[:, 1:]  # B * T-1\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # get prob\n",
    "        logits = model(x).logits\n",
    "\n",
    "        losses = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.contiguous().view(-1), reduction='none').view(B, -1)\n",
    "\n",
    "        masked_losses = losses * masks \n",
    "        \n",
    "        total_losses = masked_losses.sum(dim =-1)\n",
    "        avg_losses = total_losses / masks.sum(dim=-1)\n",
    "        \n",
    "        # eval if accurate\n",
    "        num_correct += total_losses.argmin().item() == label \n",
    "        num_correct_norm += avg_losses.argmin().item() == label \n",
    "        num_total += 1\n",
    "\n",
    "    print(f\"evaluated {num_total} examples: {num_correct_norm} correct using avg prob. {num_correct} correct using total prob\")\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return num_correct_norm, num_correct, num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluated 49 examples: 16 correct using avg prob. 17 correct using total prob\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 17, 49)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_hellaswag(iterator, model, encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
