{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce data load in train_gpt2.py\n",
    "\n",
    "**learning**\n",
    "\n",
    "L1. although each raw data text starts with eco, it is not true for each mini-batch\n",
    "\n",
    "L2. data loader only saves one shard in memory, which is the point of using shard\n",
    "\n",
    "\n",
    "\n",
    "**question**\n",
    "\n",
    "Q1. data loader self.current_position += B * T or B * T +1 \n",
    "\n",
    "A1. B*T\n",
    "\n",
    "The one-token overlap between y(t-1) and x(t) ensures that no token transition is skipped. By advancing the window by B*T but using B*T + 1 tokens per batch, the model captures all sequential relationships in the data — every token and its next-token prediction is used exactly once.\n",
    "\n",
    "Example: shard = [1,3,4,2,10,11] where B=1 and T = 2\n",
    " \n",
    "\n",
    "  first batch: x [1,3] y [3,4]\n",
    "\n",
    "second batch: x [4,2] y [2,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11 shards\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from config import local_dir\n",
    "\n",
    "\n",
    "data_dir = local_dir\n",
    "\n",
    "try:\n",
    "    contents = os.listdir(data_dir)\n",
    "except:\n",
    "    print(\"no local dir created, please run 1_fineweb_re1.ipynb first\")\n",
    "\n",
    "if len(contents) == 0:\n",
    "    print(\"no shards created, please run 1_fineweb_re1.ipynb first\")\n",
    "else:\n",
    "    print(f\"found {len(contents)} shards\")\n",
    "\n",
    "example_shard_path = contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokens to tensor\n",
    "data  = np.load(os.path.join(data_dir, example_shard_path))\n",
    "\n",
    "data_tensor =torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "data_tensor.shape == data.shape\n",
    "\n",
    "\n",
    "# create function\n",
    "def _load_tokens(filename): # L1\n",
    "\n",
    "    \"\"\"function to load tokens from a file and convert to tensor, used in data loader\n",
    "    \"\"\"\n",
    "\n",
    "    if data_dir not in filename:\n",
    "        filename = os.path.join(data_dir, filename)\n",
    "\n",
    "    data = np.load(filename)\n",
    "    # data = data.astype(np.int32) # for values strictly within the uint16 range (0–65535), converting directly to torch.long without the intermediate np.int32 works fine.\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, B:int, T:int, data_dir:str, split:str):\n",
    "\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.all_shards_paths = [ os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if split in filename]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.current_position = 0\n",
    "        self.current_shard_index = 0\n",
    "        self.current_shard = _load_tokens(self.all_shards_paths[self.current_shard_index])  # L2\n",
    "\n",
    "    def next_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        delta_n_tokens = self.B*self.T+1 \n",
    "        batch = self.current_shard[self.current_position: self.current_position + delta_n_tokens]\n",
    "        x = batch[:-1].view(self.B,self.T)        \n",
    "        y = batch[1:].view(self.B,self.T)\n",
    "        self.current_position += delta_n_tokens - 1 # Q1\n",
    "\n",
    "        # evaluate if need to load next shard\n",
    "        if self.current_position + delta_n_tokens > len(self.current_shard):\n",
    "            self.current_shard_index = (self.current_shard_index + 1) % len(self.all_shards_paths)\n",
    "            self.current_shard = _load_tokens(self.all_shards_paths[self.current_shard_index])\n",
    "            self.current_position = 0\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10416,   351,   663, 18875],\n",
      "        [ 6770,   357,    69,   451]])\n",
      "tensor([[  351,   663, 18875,  6770],\n",
      "        [  357,    69,   451,   286]])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(B=2, T=4, data_dir=data_dir, split='train')\n",
    "x, y = data_loader.next_batch()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
