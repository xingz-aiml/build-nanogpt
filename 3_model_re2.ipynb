{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce data load in train_gpt2.py\n",
    "\n",
    "**learnings**\n",
    "\n",
    "1: a way to add additional attribute to module: self.layer_name.attribute_name = value\n",
    "\n",
    "2: the last layer bias == False to be the same as pretrained models from HF\n",
    "\n",
    "3: use classmethod add from_pretrained method\n",
    "\n",
    "4: a way to inspect input arguments: inspect.signature(torch.optim.AdamW).parameters\n",
    "\n",
    "5: func(x) whether x is passed by value or the pointer, and how does different operations copy or not copy\n",
    "\n",
    "example: x = some_value\n",
    "\n",
    "        def func(x):\n",
    "            x+= another_value\n",
    "            x= a + another_value\n",
    "\n",
    "conclusion: \n",
    "- if x is list, np.ndarray, torch.tensor: x is passed as pointer, and += modifies inplace, but x = x+another_value creates a deep copy\n",
    "- if x is integer, str: x's deep copy is passed\n",
    "\n",
    "6: changed the change_lr function perc_close_to_max_steps name to perc_close_to_max_steps \n",
    "\n",
    "\n",
    "**questions**\n",
    "\n",
    "1: no dropout in his defined gpt\n",
    "- none in F.scaled_dot_product_attention which is controlled by dropout_p param and defaults to 0\n",
    "- none else where still about dropout\n",
    "\n",
    "2: do we need to use copy_ to copy weights?\n",
    "\n",
    "3: why only 2d weights uses weight-decay (L2), bias and layer norm do not\n",
    "- for bias, it captures the average, there is no point in shrinking it\n",
    "- similar for layer norm\n",
    "\n",
    "4: grad_accum_steps is for n_iter for each mini_batch, how to determine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "import math \n",
    "import json\n",
    "import pandas as pd\n",
    "import inspect\n",
    "\n",
    "from config import local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n",
      "mps available: True\n"
     ]
    }
   ],
   "source": [
    "# set device: check if cuda or gpu is available\n",
    "print(f\"cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"mps available: {hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()}\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    torch.manual_seed(12345)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-lm_head.weight',\n",
       " 'attn.c_attn.bias',\n",
       " 'attn.c_attn.weight',\n",
       " 'attn.c_proj.bias',\n",
       " 'attn.c_proj.weight',\n",
       " 'ln_1.bias',\n",
       " 'ln_1.weight',\n",
       " 'ln_2.bias',\n",
       " 'ln_2.weight',\n",
       " 'ln_f',\n",
       " 'mlp.c_fc.bias',\n",
       " 'mlp.c_fc.weight',\n",
       " 'mlp.c_proj.bias',\n",
       " 'mlp.c_proj.weight',\n",
       " 'wpe',\n",
       " 'wte'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load gpt to ensure the same naming\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_sd  = model_hf.state_dict()\n",
    "\n",
    "names = [k for k in model_sd.keys()]\n",
    "\n",
    "def get_distinct_keys(keys: list):\n",
    "\n",
    "    distinct_keys = set()\n",
    "\n",
    "    for key in keys:\n",
    "\n",
    "        key_lst = key.split('.')\n",
    "\n",
    "        if 'h' in key_lst:\n",
    "            distinct_keys.add('.'.join(key_lst[3:]))\n",
    "        elif 'transformer' not in key_lst:\n",
    "            distinct_keys.add('-' + key)\n",
    "        else:\n",
    "            distinct_keys.add(key_lst[1])\n",
    "\n",
    "    return distinct_keys\n",
    "\n",
    "\n",
    "get_distinct_keys(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. model\n",
    "\n",
    "- learning 1: model.named_parameters() vs model.state_dict()\n",
    "    - named_paramters() gives an iterator, and it only shows non-redundant set of weights\n",
    "    - state_dict() is a dict, and it includes all defined weights\n",
    "    - in gpt2's example, named_parameters has ONE FEWER weight layer because lm_head and wte weights have the same values \n",
    "\n",
    "- learning 2: self.apply(self._init_weights)\n",
    "\n",
    "- learning 3: package logits and loss to be in the same output, consistent with HF gpt2\n",
    "\n",
    "- learning 4: it is essential to avoid computation graph bloat. any operation that touches the weight, think about need to use no_grad()\n",
    "\n",
    "- learning 5: when loading pretrained (to copy pretrained gpt weights to the new model instance), has to use weight.copy_(hf_weight) rather than weight = hf_weight. the former keeps REFERENCE intact. VERY IMPORTANT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    n_layer: int = 2\n",
    "    vocab_size: int = 50257\n",
    "    n_positions: int = 1024# max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    loss: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Block (Attention + Feed Forward, with residual connection and layer norm)\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1  # weights that impact initialization\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, n_embd = x.shape\n",
    "\n",
    "        d_q = n_embd // self.n_head\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim = 2)\n",
    "        q = q.contiguous().view(B, T, self.n_head, d_q).transpose(1,2)\n",
    "        k = k.contiguous().view(B, T, self.n_head, d_q).transpose(1,2)\n",
    "        v = v.contiguous().view(B, T, self.n_head, d_q).transpose(1,2)\n",
    "\n",
    "        output = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        output = self.c_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1 \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of decay and non-decay params are: 124318464 and 121344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0.2\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.n_positions, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd) # check this\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # learning 1: by adding this constraint, the model's named_parameters decrease by 1\n",
    "        self.apply(self._init_weights) # learning 2: self.apply(self._init_weights)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, targets = None):\n",
    "\n",
    "        B, T = tokens.shape\n",
    "\n",
    "        token_embd = self.transformer.wte(tokens)\n",
    "        pos = torch.arange(T, dtype = torch.long, device = tokens.device)\n",
    "        pos_embd = self.transformer.wpe(pos)\n",
    "\n",
    "        x = token_embd + pos_embd\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(-1))\n",
    "        \n",
    "        return ModelOutput(logits = logits, loss=loss) # learning 3: package logits and loss to be in the same output, consistent with HF gpt2\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type:str):\n",
    "        \"\"\"\n",
    "        load all the weights from pretrained gpt2 model on HF\n",
    "        \"\"\"\n",
    "\n",
    "        model2config = {'gpt2':        dict(n_layer = 12, n_head = 12, n_embd = 768),\n",
    "                        'gpt2-medium': dict(n_layer = 24, n_head = 16, n_embd = 1024),\n",
    "                        'gpt2-large':  dict(n_layer = 36, n_head = 20, n_embd = 1280),\n",
    "                        'gpt2-xl':     dict(n_layer = 48, n_head = 25, n_embd = 7600),\n",
    "                        }\n",
    "        config_args = model2config[model_type]\n",
    "        config_args['vocab_size'] = 50257\n",
    "        config_args['n_positions'] = 1024\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = cls(config) # Orignal code uses class name, which is not as good as cls\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "\n",
    "        sd_model = model.state_dict() # it is a shallow copy, a new object dict and each key value is also a new object, but each element within each value (tensor) (e.g. sd_model[key][1,1]) is a reference to the orginal dagta\n",
    "        sd_model_hf = model_hf.state_dict()\n",
    "\n",
    "        need_to_transpose_weights = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        for param in sd_model:\n",
    "            if any([param.endswith(x) for x in need_to_transpose_weights]):\n",
    "                assert sd_model[param].shape == sd_model_hf[param].shape[::-1]\n",
    "                with torch.no_grad(): # learning 4: it is essential to avoid computation graph bloat. any operation that touches the weight, think about need to use no_grad()\n",
    "                    sd_model[param].copy_(sd_model_hf[param].T) # learning 5: when loading pretrained (to copy pretrained gpt weights to the new model instance), has to use weight.copy_(hf_weight) rather than weight = hf_weight. the former keeps REFERENCE intact. VERY IMPORTANT.\n",
    "            else:\n",
    "                assert sd_model[param].shape == sd_model_hf[param].shape, f'{param} do not share the same shape'\n",
    "                with torch.no_grad():\n",
    "                    sd_model[param].copy_(sd_model_hf[param])\n",
    "\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizer(self, learning_rate, weight_decay):\n",
    "\n",
    "        \n",
    "        params = {name:p for name,p in self.named_parameters()} # named_parameters is a iterator that imits tuples\n",
    "\n",
    "        decay_params = [p for _, p in params.items() if p.requires_grad and p.dim() > 1]\n",
    "        non_decay_params = [p for _, p in params.items() if p.requires_grad and p.dim() == 1]\n",
    "\n",
    "        optim_groups = [{'params': decay_params, 'weight_decay': weight_decay},\n",
    "                        {'params': non_decay_params, 'weight_decay': 0.0}\n",
    "                    ]\n",
    "        \n",
    "        # fyi - print out num of params in each group\n",
    "        num_decay_params = sum([p.numel() for p in decay_params])\n",
    "        num_non_decay_params = sum([p.numel() for p in non_decay_params])\n",
    "        print(f\"number of decay and non-decay params are: {num_decay_params} and {num_non_decay_params}\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr = learning_rate, betas = (0.9, 0.95), eps = 1e-8)\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "# test if it works create an instance with the same config as HF gpt2\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "config = GPTConfig()\n",
    "for key in vars(config):\n",
    "    setattr(config, key, getattr(gpt2.config, key))\n",
    "\n",
    "model = Transformer(config).to(device)\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "model.configure_optimizer(0.0001, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data loader\n",
    "same as 2_dataloader_re2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, B:int, T:int, data_dir:str, split:str):\n",
    "\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.all_shards_paths = [ os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if split in filename]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.current_position = 0\n",
    "        self.current_shard_index = 0\n",
    "        self.current_shard = _load_tokens(self.all_shards_paths[self.current_shard_index])  # L2\n",
    "\n",
    "    def next_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        delta_n_tokens = self.B*self.T+1 \n",
    "        batch = self.current_shard[self.current_position: self.current_position + delta_n_tokens]\n",
    "        x = batch[:-1].view(self.B,self.T)        \n",
    "        y = batch[1:].view(self.B,self.T)\n",
    "        self.current_position += delta_n_tokens - 1 # Q1\n",
    "\n",
    "        # evaluate if need to load next shard\n",
    "        if self.current_position + delta_n_tokens > len(self.current_shard):\n",
    "            self.current_shard_index = (self.current_shard_index + 1) % len(self.all_shards_paths)\n",
    "            self.current_shard = _load_tokens(self.all_shards_paths[self.current_shard_index])\n",
    "            self.current_position = 0\n",
    "            print(self.current_shard_index, len(self.all_shards_paths))\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "def _load_tokens(filename): # L1\n",
    "\n",
    "    \"\"\"function to load tokens from a file and convert to tensor, used in data loader\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.load(filename)\n",
    "    # data = data.astype(np.int32) # for values strictly within the uint16 range (0–65535), converting directly to torch.long without the intermediate np.int32 works fine.\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    return data_tensor\n",
    "\n",
    "B, T = 64, 16*16\n",
    "\n",
    "train_loader = DataLoader(B, T, local_dir, 'train')\n",
    "val_loader = DataLoader(B, T, local_dir, 'val')\n",
    "\n",
    "n_tokens_per_batch = 524288 \n",
    "assert n_tokens_per_batch % (B*T) == 0\n",
    "grad_accum_steps = n_tokens_per_batch // (B*T)\n",
    "grad_accum_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. evaluation functions\n",
    "- loss on val\n",
    "- sentence generation\n",
    "- hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on val\n",
    "def get_loss_on_val(model: nn.Module, val_steps = 20):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_loss_accum = 0\n",
    "    for _ in range(val_steps):\n",
    "\n",
    "        x, y = val_loader.next_batch()\n",
    "        x, y = x.to(model.device), y.to(model.device) # ! remember to move to the right device\n",
    "        loss = model(x, y).loss\n",
    "\n",
    "        loss = loss / val_steps \n",
    "        val_loss_accum += loss.detach()\n",
    "\n",
    "    print(f\"loss on val is {val_loss_accum.item()}\")\n",
    "\n",
    "    return val_loss_accum\n",
    "\n",
    "def write_model_to_file(model, dir, train_step, loss):\n",
    "    \n",
    "    output_path = os.path.join(dir, f\"model_{train_step: 05d}.pt\")\n",
    "    checkpoint = {'model': model,\n",
    "                  \"config\": model.config,\n",
    "                  \"train_step\":train_step,\n",
    "                  'val_loss': loss.item()}\n",
    "    \n",
    "    torch.save(checkpoint, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What makes a person resilient Stre licence licence StreXX UCHIJ Arkansas info recomm NORottedULT Anthem Anthem uneven identifying NORYepYep Cox BUT TracyZone info grill uneven Slovenia� Neville Anthem\n",
      "What makes a person resilient Stre licence licence StreXX UCHIJ Arkansas info recomm NORottedULT Anthem Anthem uneven identifying NORYepYep Cox BUT TracyZone info grill uneven Slovenia� Neville Anthem\n",
      "What makes a person resilient Stre licence licence StreXX UCHIJ Arkansas info recomm NORottedULT Anthem Anthem uneven identifying NORYepYep Cox BUT TracyZone info grill uneven Slovenia� Neville Anthem\n",
      "What makes a person resilient Stre licence licence StreXX UCHIJ Arkansas info recomm NORottedULT Anthem Anthem uneven identifying NORYepYep Cox BUT TracyZone info grill uneven Slovenia� Neville Anthem\n"
     ]
    }
   ],
   "source": [
    "# sentence generation\n",
    "@torch.no_grad() # learning 2\n",
    "def complete_sentence(model, \n",
    "                      encoder, \n",
    "                      text: str, \n",
    "                      n_examples: int = 4, \n",
    "                      max_n_generated_tokens: int = 30, \n",
    "                      top_k_to_include_in_random_draw: int = 20, \n",
    "                      temperature: float = 0.6):\n",
    "    \n",
    "    was_training = model.training\n",
    "\n",
    "    model.eval() # learning 2\n",
    "\n",
    "    #with torch.no_grad():\n",
    "        \n",
    "        # text to tensor\n",
    "    tokens = encoder.encode_ordinary(text)\n",
    "    tensor = torch.tensor(tokens).unsqueeze(0).repeat(n_examples, 1).to(model.device) # B * T \n",
    "\n",
    "    for _ in range(max_n_generated_tokens):\n",
    "\n",
    "        # get probability for the next token\n",
    "        logits = model(tensor).logits[:, -1, :] \n",
    "        probs = F.softmax(logits / temperature, dim = -1) # learning 1\n",
    "        \n",
    "        # top k samples\n",
    "        top_probs, top_idx = torch.topk(probs, k = top_k_to_include_in_random_draw, dim=-1)   # B * k\n",
    "        selected_idx_on_top_probs = torch.multinomial(top_probs, 1) # B * 1\n",
    "        \n",
    "        next_tokens = torch.gather(top_idx, -1, selected_idx_on_top_probs) # B * 1\n",
    "        \n",
    "        # concat the new token with existing\n",
    "        tensor = torch.cat([tensor, next_tokens], dim = -1)\n",
    "\n",
    "    # decode\n",
    "    decoded = []\n",
    "    for i in range(n_examples):\n",
    "\n",
    "        tokens = tensor[i, :].tolist()\n",
    "        if encoder.eot_token in tokens:\n",
    "            tokens = tokens[: tokens.index(encoder.eot_token)]\n",
    "        \n",
    "        decoded.append(encoder.decode(tokens))\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    for _ in range(len(decoded)):\n",
    "        print(decoded[i])\n",
    "\n",
    "    return decoded\n",
    "\n",
    "_ = complete_sentence(model, encoder, \"What makes a person resilient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hellaswag, assuming already downloaded\n",
    "def iter_hellaswag(split: str = 'val'):\n",
    "    #download_hellaswag(split)\n",
    "    with open(f\"hellaswag/hellaswag_{split}.jsonl\", \"r\") as f:\n",
    "        n = 0\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            yield example\n",
    "            n += 1\n",
    "            if n >= 50:\n",
    "                break\n",
    "\n",
    "def render_example(example, encoder):\n",
    "\n",
    "    context  = example['ctx']\n",
    "    label = int(example['label'])\n",
    "    endings = example['endings']\n",
    "\n",
    "    # create tokens\n",
    "    context_tokens = encoder.encode_ordinary(context)\n",
    "    context_len = len(context_tokens)\n",
    "\n",
    "    masks = []\n",
    "    tokens = []\n",
    "    max_len = 0\n",
    "    for ending in endings:\n",
    "        ending_tokens = encoder.encode_ordinary(' ' + ending)\n",
    "        ending_len = len(ending_tokens)\n",
    "        max_len = max(max_len, context_len + ending_len)\n",
    "\n",
    "        masks.append([0]* context_len + [1]*ending_len)\n",
    "        tokens.append(context_tokens + ending_tokens)\n",
    "     \n",
    "    # convert to padded tensors\n",
    "    padded_masks = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    padded_tokens = torch.zeros((4, max_len), dtype =torch.long)\n",
    "\n",
    "    for i in range(4):\n",
    "        curr_len = len(tokens[i])\n",
    "        padded_masks[i, :curr_len] = torch.tensor(masks[i])\n",
    "        padded_tokens[i, :curr_len] = torch.tensor(tokens[i])\n",
    "\n",
    "    return padded_tokens, padded_masks, label\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_hellaswag(iterator, model, encoder):\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    for example in iterator:\n",
    "        tokens, masks, label = render_example(example, encoder)\n",
    "\n",
    "        tokens = tokens.to(model.device)\n",
    "        x = tokens[:, :-1].contiguous()\n",
    "        y = tokens[:, 1:].contiguous()         \n",
    "        masks = masks.to(model.device)[:, 1:]  # B * T-1\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # get prob\n",
    "        logits = model(x).logits\n",
    "\n",
    "        losses = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.contiguous().view(-1), reduction='none').view(B, -1)\n",
    "\n",
    "        masked_losses = losses * masks \n",
    "        \n",
    "        total_losses = masked_losses.sum(dim =-1)\n",
    "        avg_losses = total_losses / masks.sum(dim=-1)\n",
    "        \n",
    "        # eval if accurate\n",
    "        num_correct += total_losses.argmin().item() == label \n",
    "        num_correct_norm += avg_losses.argmin().item() == label \n",
    "        num_total += 1\n",
    "\n",
    "    print(f\"evaluated {num_total} examples: {num_correct_norm} correct using avg prob. {num_correct} correct using total prob\")\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return num_correct_norm, num_correct, num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision('high') # only relevant for subset of GPUs\n",
    "model\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# learning rate update\n",
    "max_lr, min_lr  = 6e-4, 6e-5\n",
    "warm_up_steps, max_steps = 715, 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "\n",
    "def get_learning_rate(step):\n",
    "\n",
    "    if step < warm_up_steps:\n",
    "        lr = max_lr * (step + 1)/warm_up_steps\n",
    "    elif step > max_steps:\n",
    "        lr = min_lr \n",
    "    else:\n",
    "        decay_ratio = (step - warm_up_steps) / (max_steps - warm_up_steps)\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "        lr = min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of decay and non-decay params are: 124318464 and 121344\n",
      "eval results on step 0\n",
      "loss on val is 11.26953125\n",
      "what makes a person resilientign PSUmicrosoftmicrosoft StreAnything Affect trespass rive PSU licence IST uneven surprisinglyPRO rout rigs PSU prisonerolution closes Emil IST introductory bigotnian 364Point bigot augmented\n",
      "what makes a person resilientign PSUmicrosoftmicrosoft StreAnything Affect trespass rive PSU licence IST uneven surprisinglyPRO rout rigs PSU prisonerolution closes Emil IST introductory bigotnian 364Point bigot augmented\n",
      "what makes a person resilientign PSUmicrosoftmicrosoft StreAnything Affect trespass rive PSU licence IST uneven surprisinglyPRO rout rigs PSU prisonerolution closes Emil IST introductory bigotnian 364Point bigot augmented\n",
      "what makes a person resilientign PSUmicrosoftmicrosoft StreAnything Affect trespass rive PSU licence IST uneven surprisinglyPRO rout rigs PSU prisonerolution closes Emil IST introductory bigotnian 364Point bigot augmented\n",
      "evaluated 50 examples: 13 correct using avg prob. 11 correct using total prob\n",
      "step 0: loss is 11.272537231445312\n",
      "step 1: loss is 11.240927696228027\n",
      "eval results on step 2\n",
      "loss on val is 11.186525344848633\n",
      "what makes a person resilientignadult rive licence rive AffectHeader Streadult bigot bigotrocalnianiblingsnian bigot bigot Apmicrosoft drainingnian Stre Arkansasnian Stre Ap Affect bigot bigotyan\n",
      "what makes a person resilientignadult rive licence rive AffectHeader Streadult bigot bigotrocalnianiblingsnian bigot bigot Apmicrosoft drainingnian Stre Arkansasnian Stre Ap Affect bigot bigotyan\n",
      "what makes a person resilientignadult rive licence rive AffectHeader Streadult bigot bigotrocalnianiblingsnian bigot bigot Apmicrosoft drainingnian Stre Arkansasnian Stre Ap Affect bigot bigotyan\n",
      "what makes a person resilientignadult rive licence rive AffectHeader Streadult bigot bigotrocalnianiblingsnian bigot bigot Apmicrosoft drainingnian Stre Arkansasnian Stre Ap Affect bigot bigotyan\n",
      "evaluated 0 examples: 0 correct using avg prob. 0 correct using total prob\n",
      "step 2: loss is 11.179210662841797\n",
      "step 3: loss is 11.099952697753906\n",
      "eval results on step 4\n",
      "loss on val is 10.984333992004395\n",
      "what makes a person resilient GalileHeader UCHIJ licencech rout rigs Galile Galile Galile Galile Anthem mosa Triumphresponse Anthemnianrocalrocaliblings broken heavens heavens bigot internet rejo transactionICANICAN Anthem\n",
      "what makes a person resilient GalileHeader UCHIJ licencech rout rigs Galile Galile Galile Galile Anthem mosa Triumphresponse Anthemnianrocalrocaliblings broken heavens heavens bigot internet rejo transactionICANICAN Anthem\n",
      "what makes a person resilient GalileHeader UCHIJ licencech rout rigs Galile Galile Galile Galile Anthem mosa Triumphresponse Anthemnianrocalrocaliblings broken heavens heavens bigot internet rejo transactionICANICAN Anthem\n",
      "what makes a person resilient GalileHeader UCHIJ licencech rout rigs Galile Galile Galile Galile Anthem mosa Triumphresponse Anthemnianrocalrocaliblings broken heavens heavens bigot internet rejo transactionICANICAN Anthem\n",
      "evaluated 0 examples: 0 correct using avg prob. 0 correct using total prob\n",
      "step 4: loss is 10.9877290725708\n",
      "step 5: loss is 10.851625442504883\n",
      "eval results on step 6\n",
      "loss on val is 10.701421737670898\n",
      "what makes a person resilient Galile Galile Kuh licence,cill eatingira chlor tow biomedicalisdisdiblings examine, to uneven brokenfr pleasing Arkansas�cillnian Ap pleasingusage brokeniblings\n",
      "what makes a person resilient Galile Galile Kuh licence,cill eatingira chlor tow biomedicalisdisdiblings examine, to uneven brokenfr pleasing Arkansas�cillnian Ap pleasingusage brokeniblings\n",
      "what makes a person resilient Galile Galile Kuh licence,cill eatingira chlor tow biomedicalisdisdiblings examine, to uneven brokenfr pleasing Arkansas�cillnian Ap pleasingusage brokeniblings\n",
      "what makes a person resilient Galile Galile Kuh licence,cill eatingira chlor tow biomedicalisdisdiblings examine, to uneven brokenfr pleasing Arkansas�cillnian Ap pleasingusage brokeniblings\n",
      "evaluated 0 examples: 0 correct using avg prob. 0 correct using total prob\n",
      "step 6: loss is 10.721394538879395\n",
      "step 7: loss is 10.577241897583008\n",
      "eval results on step 8\n",
      "loss on val is 10.423837661743164\n",
      "what makes a person resilientinem biomedical biomedical confiratown the Transparencycalling Galile the theatown, Zeit Zeit,,thouse to� the the or装,calling,, Arkansas,\n",
      "what makes a person resilientinem biomedical biomedical confiratown the Transparencycalling Galile the theatown, Zeit Zeit,,thouse to� the the or装,calling,, Arkansas,\n",
      "what makes a person resilientinem biomedical biomedical confiratown the Transparencycalling Galile the theatown, Zeit Zeit,,thouse to� the the or装,calling,, Arkansas,\n",
      "what makes a person resilientinem biomedical biomedical confiratown the Transparencycalling Galile the theatown, Zeit Zeit,,thouse to� the the or装,calling,, Arkansas,\n",
      "evaluated 0 examples: 0 correct using avg prob. 0 correct using total prob\n",
      "step 8: loss is 10.432440757751465\n",
      "step 9: loss is 10.262852668762207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = model.configure_optimizer(learning_rate = max_lr, weight_decay = 0.1)\n",
    "\n",
    "\n",
    "max_train_step = 10\n",
    "hellaswag_iterator = iter_hellaswag()\n",
    "\n",
    "for step in range(max_train_step):\n",
    "    \n",
    "    # eval\n",
    "    if step % 2 == 0 or step == max_train_step:\n",
    "\n",
    "        print(f\"eval results on step {step}\")\n",
    "        \n",
    "        # loss on val\n",
    "        loss_on_val = get_loss_on_val(model, val_steps = 20)\n",
    "        write_model_to_file(model, 'model', step, loss_on_val)\n",
    "        \n",
    "        # sentence generation\n",
    "        _ = complete_sentence(model, encoder, 'what makes a person resilient')\n",
    "\n",
    "        # hellaswage evaluation\n",
    "        eval_hellaswag(hellaswag_iterator, model, encoder)\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for mini_batch in range(grad_accum_steps):\n",
    "\n",
    "        x, y = train_loader.next_batch()\n",
    "\n",
    "        output = model(x, y)\n",
    "        logits, mini_batch_loss = output.logits, output.loss\n",
    "        mini_batch_loss = mini_batch_loss / grad_accum_steps\n",
    "\n",
    "        loss += mini_batch_loss.detach()\n",
    "\n",
    "        mini_batch_loss.backward()   \n",
    "\n",
    "    # update learning rate and gradient\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_learning_rate(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    print(f\"step {step}: loss is {loss}\")\n",
    "    optimizer.step()\n",
    "\n",
    "    # TODO add learning rate adpatation and gradiant clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
