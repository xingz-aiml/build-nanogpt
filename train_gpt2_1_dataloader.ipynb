{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce data load in train_gpt2.py\n",
    "\n",
    "discussion 1\n",
    "- data loader self.current_position += n_tokens_in_batch or +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edufineweb_train_000008.npy',\n",
       " 'edufineweb_val_000000.npy',\n",
       " 'edufineweb_train_000009.npy',\n",
       " 'edufineweb_train_000001.npy',\n",
       " 'edufineweb_train_000002.npy',\n",
       " 'edufineweb_train_000003.npy',\n",
       " 'edufineweb_train_000007.npy',\n",
       " 'edufineweb_train_000006.npy',\n",
       " 'edufineweb_train_000004.npy',\n",
       " 'edufineweb_train_000010.npy',\n",
       " 'edufineweb_train_000005.npy']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data_dir = 'edu_fineweb10B'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_tokens\n",
    "data  = np.load(os.path.join(data_dir,'edufineweb_val_000000.npy'))\n",
    "#data = data.astype(np.int32) \n",
    "\n",
    "data_tensor = torch.tensor(data, dtype = torch.long) \n",
    "data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "\n",
    "    data = np.load(filename)\n",
    "    data = data.astype(np.int32)\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    return data_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, split, data_dir):\n",
    "\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.split = split \n",
    "        assert split in {'train','val'}\n",
    "\n",
    "        self.shards = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if split in filename]\n",
    "        assert len(self.shards) > 0, \"no shards\"\n",
    "\n",
    "        print(f\"{len(self.shards)} shards for {split}\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T \n",
    "        n_tokens_in_batch = B*T\n",
    "        buf = self.tokens[self.current_position: self.current_position+n_tokens_in_batch+1]\n",
    "        x = buf[:-1].view(B,T)\n",
    "        y = buf[1:].view(B,T)\n",
    "\n",
    "        self.current_position += n_tokens_in_batch #??\n",
    "        \n",
    "        # if the remaining is not enough to be a batch, skip the remaining and advance to the next shard\n",
    "        if self.current_position + n_tokens_in_batch + 1 > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.current_position = 0\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 shards for val\n"
     ]
    }
   ],
   "source": [
    "B = 2\n",
    "T = 10\n",
    "train_loader = DataLoaderLite(B, T, 'val', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50256,   464, 13362, 12091,   198,  1890,   477,   262,  1842,    11],\n",
       "         [19661,   290, 10731,   287, 12091,  2517,   268,   447,   247,    82]]),\n",
       " tensor([[  464, 13362, 12091,   198,  1890,   477,   262,  1842,    11, 19661],\n",
       "         [  290, 10731,   287, 12091,  2517,   268,   447,   247,    82,  3835]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3835,    11,   644,   484,   389,  1107,   546,   318,  4925,   290],\n",
       "         [10404,    13, 20153,   286,  1807,   290,   262,  4925,   284,  3853]]),\n",
       " tensor([[   11,   644,   484,   389,  1107,   546,   318,  4925,   290, 10404],\n",
       "         [   13, 20153,   286,  1807,   290,   262,  4925,   284,  3853,    13]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
