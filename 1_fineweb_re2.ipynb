{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**goal**\n",
    "\n",
    "data preparation: generate tokenized np array for train and eval\n",
    "\n",
    "1. download a sampled version from [HuggingFaceFW/fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n",
    "2. load encoder and tokenize  \n",
    "3. save as np array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "import multiprocessing as mp\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = int(1e7)\n",
    "\n",
    "# LOCAL dir to save the cached data\n",
    "local_dir = \"edu_fineweb10B\"\n",
    "os.makedirs(local_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. download a sampled version from [HuggingFaceFW/fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n",
    "\n",
    "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d1588c84ac47798ee0c80a8d7f2539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef77eda361c3411fa7473e2df32a2ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object type <class 'datasets.arrow_dataset.Dataset'>\n",
      "# of data points 9,672,101\n",
      "one data point\n",
      "{'text': 'The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what they are really about is freedom and independence. Independence of thought and the freedom to choose.\\nElizabeth’s refusal of Mr. Collins offer of marriage showed an independence seldom seen in heroines of the day. Her refusal of Mr. Darcy while triggered by anger showed a level of independence that left him shocked and stunned.\\nThe freedom she exhibited in finally accepting him in direct defiance of Lady Catherine and knowing her father would disapprove was unusual even for Austen. In her last book Anne Elliot is persuaded to refuse Captain Wentworth at Lady Russel’s insistence.\\nAlthough Jane played by the rules of the day, all of her writing is infused with how she wanted life to be. She ‘screams’ her outrage at the limitations for women in Emma.\\nWhen accosted by Mrs. Elton, Jane Fairfax says,\\n“Excuse me, ma’am, but this is by no means my intention; I make no inquiry myself, and should be sorry to have any made by my friends. When I am quite determined as to the time, I am not at all afraid of being long unemployed. There are places in town, offices, where inquiry would soon produce something — offices for the sale, not quite of human flesh, but of human intellect.”\\n“Oh! my dear, human flesh! You quite shock me; if you mean a fling at the slave-trade, I assure you Mr. Suckling was always rather a friend to the abolition.”\\n“I did not mean, I was not thinking of the slave-trade,” replied Jane; “governess-trade, I assure you, was all that I had in view; widely different certainly, as to the guilt of those who carry it on; but as to the greater misery of the victims, I do not know where it lies.”\\nThat same sentiment is emphasized in Emma’s shock when Mrs. Weston tells her of Frank Churchill’s secret engagement to Jane.\\n“Good God!” cried Emma, “Jane actually on the point of going as governess! What could he mean by such horrible indelicacy? To suffer her to engage herself — to suffer her even to think of such a measure!”\\nI find it interesting that at the moment of Austen’s birth or there about, John Adams left his farm in Massachusetts for the Continental Congress in Philadelphia. Doesn’t sound particularly interesting, I know but consider this.\\nJohn Adams left his home in mid-December 1775 to attend an unprecedented meeting of colonial representatives to consider severing ties with their mother country and her monarch; a decision that culminated in a document unlike any ever written. In the mother country, one day in that same cold December a baby girl was born at Steventon Rectory. Her cry was heard by only the people in the house but the years to come would see her pen create works unlike any the world had ever seen.\\nComparing Austen’s words with Thomas Jefferson’s may seem a trivialization but I believe that Austen’s impact on the world is no less important than Jefferson’s. The effect of Jane’s writing maybe more subtle than that of the Virginian but it is no less influential.\\nJefferson’s words instigated and promoted a revolution, a war of independence. Jane’s words had no such excessive consequence. Still in her own quiet, genteel yet powerful way she declared and promoted the same principles of freedom and self-regulated independence as our American forefathers. In all her novels Jane advocates independence of person and thought, the rights of all and acceptance of responsibility for those rights.\\nJane may not have incited military action as Jefferson did but even as an avowed royalist, I doubt not that Jane Austen firmly believed in his declaration of the right to life, liberty and the pursuit of happiness.', 'id': '<urn:uuid:0d8a309d-25c5-405d-a08a-c11239f0d717>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://austenauthors.net/the-independent-jane', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9743200540542603, 'token_count': 845, 'score': 2.75, 'int_score': 3}\n"
     ]
    }
   ],
   "source": [
    "# HF datapath \n",
    "datapath = \"HuggingFaceFW/fineweb-edu\"\n",
    "sub_name = \"sample-10BT\"\n",
    "\n",
    "fw = load_dataset(datapath, sub_name, split = 'train')\n",
    "\n",
    "print(\"object type\", type(fw))\n",
    "print(\"# of data points\", f\"{len(fw):,}\")\n",
    "print(\"one data point\\n\", fw[0], sep = '')\n",
    "\n",
    "n_sample = int(len(fw) * 0.01)\n",
    "\n",
    "fw_sample = [{'text': fw[i]['text']} for i in range(n_sample)]\n",
    "del fw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  load encoder and tokenize func (removed to utils due to multi-thread process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>vars</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all vars:\n",
      "['name', '_pat_str', '_mergeable_ranks', '_special_tokens', 'max_token_value', '_core_bpe', 'special_tokens_set']\n",
      "\n",
      "max token value:\n",
      "50256\n",
      "\n",
      "special tokens:\n",
      "{'<|endoftext|>': 50256}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>dir</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callable functions:\n",
      "['decode', 'decode_batch', 'decode_bytes', 'decode_bytes_batch', 'decode_single_token_bytes', 'decode_tokens_bytes', 'decode_with_offsets', 'encode', 'encode_batch', 'encode_ordinary', 'encode_ordinary_batch', 'encode_single_token', 'encode_with_unstable', 'token_byte_values']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>encode_ordinary</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 0]\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# show stored objects \n",
    "display(HTML(f\"<h2>vars</h2>\"))\n",
    "print(\"all vars:\\n\", [k for k in vars(enc)], end = '\\n\\n', sep = '')\n",
    "print(\"max token value:\\n\", enc.max_token_value, end = '\\n\\n', sep = '')\n",
    "print(\"special tokens:\\n\", enc._special_tokens, end = '\\n\\n', sep = '')\n",
    "\n",
    "# show callable functions that are not dunder methods\n",
    "display(HTML(f\"<h2>dir</h2>\"))\n",
    "print(\"callable functions:\\n\", [k for k in dir(enc) if callable(getattr(enc, k)) and not k.startswith('_')], end = '\\n\\n', sep = '')\n",
    "\n",
    "# show the result of encode_ordinary\n",
    "display(HTML(f\"<h2>encode_ordinary</h2>\"))\n",
    "print(enc.encode_ordinary(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move tokenize to utils.py to ensure it is in __main__\n",
    "# def tokenize(doc):\n",
    "#     \"\"\"\n",
    "#     doc: string of a single document\n",
    "#     returns a numpy array of unit 16 tokens\n",
    "#     \"\"\"\n",
    " \n",
    "#     tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
    "#     tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "#     tokens_np = np.array(tokens)\n",
    "\n",
    "#     ## note: remove below token check, checking once is enough \n",
    "#     ## assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "   \n",
    "#     tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "#     return tokens_np_uint16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in each data point 869\n",
      "# of data points in each shard 11507\n",
      "# of shards, approx 8\n"
     ]
    }
   ],
   "source": [
    "# each example has about 800-1000 tokens\n",
    "n_tokens = len(enc.encode_ordinary(fw_sample[5]['text']))\n",
    "print(\"# of tokens in each data point\", n_tokens )\n",
    "print(\"# of data points in each shard\", shard_size//n_tokens)\n",
    "print(\"# of shards, approx\", len(fw_sample)*n_tokens//shard_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. write 1d array to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of cores for compute 14\n"
     ]
    }
   ],
   "source": [
    "nprocs = max(0, os.cpu_count())\n",
    "\n",
    "print(\"# of cores for compute\", nprocs)\n",
    "\n",
    "nprocs //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|█████████▉| 9999305/10000000 [01:07<00:00, 148999.59tokens/s]  \n",
      "Shard 0: 100%|█████████▉| 9999305/10000000 [00:00<00:00, 15174552.04tokens/s]\n",
      "Shard 1: 100%|█████████▉| 9999180/10000000 [00:00<00:00, 21658105.77tokens/s]\n",
      "Shard 2: 100%|█████████▉| 9999727/10000000 [00:00<00:00, 21775226.35tokens/s]\n",
      "Shard 3: 100%|█████████▉| 9999707/10000000 [00:00<00:00, 21429036.91tokens/s]\n",
      "Shard 4: 100%|█████████▉| 9999178/10000000 [00:00<00:00, 21555291.75tokens/s]\n",
      "Shard 5: 100%|█████████▉| 9999790/10000000 [00:00<00:00, 22312649.99tokens/s]\n",
      "Shard 6: 100%|█████████▉| 9999474/10000000 [00:00<00:00, 22032215.46tokens/s]\n",
      "Shard 7: 100%|█████████▉| 9999915/10000000 [00:00<00:00, 21890227.84tokens/s]\n",
      "Shard 8: 100%|█████████▉| 9999648/10000000 [00:00<00:00, 21740380.40tokens/s]\n",
      "Shard 9: 100%|█████████▉| 9999431/10000000 [00:00<00:00, 21286797.54tokens/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import tokenize\n",
    "def _write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "def write_shard_to_file(shard_index, tokens_np, local_dir):\n",
    "    split = 'val' if shard_index == 0 else 'train'\n",
    "    filename = os.path.join(local_dir, f'edufineweb_{split}_{shard_index:06d}')\n",
    "    _write_datafile(filename, tokens_np)\n",
    "\n",
    "shard_size = int(1e7) # number of tokens per shard\n",
    "chunk_size = 64\n",
    "\n",
    "with mp.Pool(nprocs) as pool:\n",
    "\n",
    "    shard_index = 0\n",
    "    token_count = 0\n",
    "    all_tokens_np = np.empty((shard_size, ), dtype = np.uint16)\n",
    "    progress_bar = tqdm(total = shard_size, unit = 'tokens', desc=f\"Shard {shard_index}\")\n",
    "\n",
    "    for tokens in pool.imap(tokenize, fw_sample, chunksize = chunk_size):\n",
    "        \n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            delta_token_count = len(tokens)\n",
    "            all_tokens_np[token_count: token_count + delta_token_count] = tokens\n",
    "            token_count += delta_token_count\n",
    "            progress_bar.update(delta_token_count)\n",
    "        else:\n",
    "            delta_token_count = shard_size - token_count\n",
    "            remain_token_count = len(tokens) - delta_token_count\n",
    "\n",
    "            all_tokens_np[token_count:] = tokens[:delta_token_count]\n",
    "            \n",
    "            # save the shard to local dir\n",
    "            write_shard_to_file(shard_index, all_tokens_np, local_dir)\n",
    "\n",
    "            # initiate for the next shard\n",
    "            shard_index += 1\n",
    "            token_count = remain_token_count\n",
    "            all_tokens_np = np.empty((shard_size, ), dtype = np.uint16)\n",
    "            all_tokens_np[:token_count] = tokens[delta_token_count: ] \n",
    "            \n",
    "            progress_bar = tqdm(total = shard_size, unit = 'tokens', desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(remain_token_count)\n",
    "\n",
    "    if token_count > 0:\n",
    "        write_shard_to_file(shard_index, all_tokens_np, local_dir)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
